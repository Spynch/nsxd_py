# nsxd_py

Учебная практика: распределённое отказоустойчивое Key-Value хранилище на Python с консенсусом RAFT.

## Возможности
- Минимальные операции KV: `PUT`, `GET`, `DELETE`.
- Лидер выбирается RAFT-алгоритмом, репликация лога между узлами.
- Персистентность: журнал команд и состояние сохраняются на диск, узлы восстанавливаются после перезапуска.
- Контейнеризация Docker + docker-compose для запуска и масштабирования кластера.

## Архитектура
- Узел RAFT поднимает HTTP API на `aiohttp`.
- Все изменения KV выполняются через лидера, который реплицирует команды на большинство.
- Для простоты лог реплицируется целиком при heartbeats (подходит для учебных экспериментов).

## Быстрый старт

```bash
docker compose up --build
```

По умолчанию стартует 3 узла:
- `node1` на `localhost:8001`
- `node2` на `localhost:8002`
- `node3` на `localhost:8003`

## API

### PUT
```bash
curl -X POST localhost:8001/kv/put \
  -H 'Content-Type: application/json' \
  -d '{"key": "foo", "value": "bar"}'
```

### GET
```bash
curl 'localhost:8001/kv/get?key=foo'
```

### DELETE
```bash
curl -X DELETE localhost:8001/kv/delete \
  -H 'Content-Type: application/json' \
  -d '{"key": "foo"}'
```

Если запрос приходит не на лидера, ответ будет `409` с адресом лидера:
```json
{"error": "not_leader", "leader": "node2:8000"}
```

### Health
```bash
curl localhost:8001/health
```

## Динамическое добавление узла

Поднимите новый контейнер и затем зарегистрируйте его на лидере:
```bash
curl -X POST localhost:8001/raft/join \
  -H 'Content-Type: application/json' \
  -d '{"peer": "node4:8000"}'
```

## Контроль отказоустойчивости

### Пауза/возврат узла
```bash
docker pause <container>
docker unpause <container>
```

### Остановка узла
```bash
docker stop <container>
```

RAFT перевыберет лидера, если большинство узлов остаётся доступным.

## Тестирование и исследование поведения

### Быстрый тест (скрипт)
Linux/macOS (bash):
```bash
scripts/test_cluster.sh
```

Windows (PowerShell):
```powershell
./scripts/test_cluster.ps1
```

Скрипт:
1. Поднимает кластер.
2. Ищет лидера.
3. Пишет значение.
4. Останавливает лидера и проверяет перевыбор.
5. Проверяет чтение после восстановления.

### Исследование границ консенсуса
- **Кластер устойчив** при доступности большинства (например, 2 из 3).
- **Кластер недоступен**, если доступно менее половины узлов (например, 1 из 3).

Для проверки:
1. Запустите `docker compose up`.
2. Найдите лидера через `/health`.
3. Остановите один узел — кластер продолжит работать.
4. Остановите два узла — кластер перестанет принимать изменения (нет кворума).
5. Поднимите узлы обратно — консенсус восстановится и лидер будет выбран снова.

## Структура проекта
```
app/raft.py        # RAFT логика, выборы, репликация
app/server.py      # HTTP API
app/storage.py     # персистентность
Dockerfile
Docker-compose.yml
```
